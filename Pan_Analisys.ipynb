{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Cleaning\n",
    "1 -  First step is to import the data using pandas and visualize it's contentes\n",
    "\n",
    "2 - After that we need to solved what to do with the NaNs for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('problem1_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>ALTURA</th>\n",
       "      <th>CAPACIDADE_(L)</th>\n",
       "      <th>COMPOSICAO</th>\n",
       "      <th>COR</th>\n",
       "      <th>FORMATO</th>\n",
       "      <th>LARGURA</th>\n",
       "      <th>MARCA</th>\n",
       "      <th>PARA_LAVA_LOUCAS</th>\n",
       "      <th>PARA_MICRO_ONDAS</th>\n",
       "      <th>PESO</th>\n",
       "      <th>PROFUNDIDADE</th>\n",
       "      <th>TEMPO_GARANTIA</th>\n",
       "      <th>TEM_FERRO_FUNDIDO</th>\n",
       "      <th>TEM_GRELHA</th>\n",
       "      <th>TEM_TAMPA</th>\n",
       "      <th>TIPO_PRODUTO</th>\n",
       "      <th>TIPO_WOK</th>\n",
       "      <th>ITEM_PRICE</th>\n",
       "      <th>INTERESTED</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SESSION_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86.709770</th>\n",
       "      <td>264220456</td>\n",
       "      <td>30.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALUMINIO</td>\n",
       "      <td>VINHO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>LA CUISINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NAO</td>\n",
       "      <td>SIM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PANELA</td>\n",
       "      <td>NAO</td>\n",
       "      <td>199.990000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73.156401</th>\n",
       "      <td>238630912</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALUMINIO</td>\n",
       "      <td>COLORIDO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>TRAMONTINA</td>\n",
       "      <td>No</td>\n",
       "      <td>no</td>\n",
       "      <td>150.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NAO</td>\n",
       "      <td>NAO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PIPOQUEIRA</td>\n",
       "      <td>NAO</td>\n",
       "      <td>105.112581</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952.331024</th>\n",
       "      <td>218228122</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INOX</td>\n",
       "      <td>INOX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>LA CUISINE</td>\n",
       "      <td>Yes</td>\n",
       "      <td>no</td>\n",
       "      <td>190.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NAO</td>\n",
       "      <td>NAO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ESPAGUETEIRA</td>\n",
       "      <td>NAO</td>\n",
       "      <td>139.990000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637.759106</th>\n",
       "      <td>253661510</td>\n",
       "      <td>49.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ALUMINIO</td>\n",
       "      <td>VERMELHO</td>\n",
       "      <td>REDONDO</td>\n",
       "      <td>41.5</td>\n",
       "      <td>TRAMONTINA</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NAO</td>\n",
       "      <td>NAO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PIPOQUEIRA</td>\n",
       "      <td>NAO</td>\n",
       "      <td>103.293333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478.531428</th>\n",
       "      <td>253661510</td>\n",
       "      <td>49.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ALUMINIO</td>\n",
       "      <td>VERMELHO</td>\n",
       "      <td>REDONDO</td>\n",
       "      <td>41.5</td>\n",
       "      <td>TRAMONTINA</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NAO</td>\n",
       "      <td>NAO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PIPOQUEIRA</td>\n",
       "      <td>NAO</td>\n",
       "      <td>103.330242</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ITEM_ID  ALTURA  CAPACIDADE_(L) COMPOSICAO       COR  FORMATO  \\\n",
       "SESSION_ID                                                                    \n",
       "86.709770   264220456    30.5             NaN   ALUMINIO     VINHO      NaN   \n",
       "73.156401   238630912    22.0             NaN   ALUMINIO  COLORIDO      NaN   \n",
       "952.331024  218228122    24.0             NaN       INOX      INOX      NaN   \n",
       "637.759106  253661510    49.5             6.0   ALUMINIO  VERMELHO  REDONDO   \n",
       "478.531428  253661510    49.5             6.0   ALUMINIO  VERMELHO  REDONDO   \n",
       "\n",
       "            LARGURA       MARCA PARA_LAVA_LOUCAS PARA_MICRO_ONDAS   PESO  \\\n",
       "SESSION_ID                                                                 \n",
       "86.709770      14.0  LA CUISINE              NaN              NaN    NaN   \n",
       "73.156401      24.0  TRAMONTINA               No               no  150.0   \n",
       "952.331024     20.0  LA CUISINE              Yes               no  190.0   \n",
       "637.759106     41.5  TRAMONTINA              Yes              NaN  120.0   \n",
       "478.531428     41.5  TRAMONTINA              Yes              NaN  120.0   \n",
       "\n",
       "            PROFUNDIDADE  TEMPO_GARANTIA TEM_FERRO_FUNDIDO TEM_GRELHA  \\\n",
       "SESSION_ID                                                              \n",
       "86.709770           50.0             3.0               NAO        SIM   \n",
       "73.156401           40.0            12.0               NAO        NAO   \n",
       "952.331024          20.0             3.0               NAO        NAO   \n",
       "637.759106          47.0             NaN               NAO        NAO   \n",
       "478.531428          47.0             NaN               NAO        NAO   \n",
       "\n",
       "            TEM_TAMPA  TIPO_PRODUTO TIPO_WOK  ITEM_PRICE  INTERESTED  \n",
       "SESSION_ID                                                            \n",
       "86.709770         1.0        PANELA      NAO  199.990000         0.0  \n",
       "73.156401         1.0    PIPOQUEIRA      NAO  105.112581         0.0  \n",
       "952.331024        1.0  ESPAGUETEIRA      NAO  139.990000         0.0  \n",
       "637.759106        1.0    PIPOQUEIRA      NAO  103.293333         1.0  \n",
       "478.531428        1.0    PIPOQUEIRA      NAO  103.330242         0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index('SESSION_ID',inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Float64Index: 180275 entries, 86.7097703783 to 222.930790647\n",
      "Data columns (total 20 columns):\n",
      "ITEM_ID              180275 non-null int64\n",
      "ALTURA               171007 non-null float64\n",
      "CAPACIDADE_(L)       76671 non-null float64\n",
      "COMPOSICAO           156978 non-null object\n",
      "COR                  170251 non-null object\n",
      "FORMATO              90011 non-null object\n",
      "LARGURA              171007 non-null float64\n",
      "MARCA                180001 non-null object\n",
      "PARA_LAVA_LOUCAS     104086 non-null object\n",
      "PARA_MICRO_ONDAS     86402 non-null object\n",
      "PESO                 98524 non-null float64\n",
      "PROFUNDIDADE         171007 non-null float64\n",
      "TEMPO_GARANTIA       122770 non-null float64\n",
      "TEM_FERRO_FUNDIDO    180275 non-null object\n",
      "TEM_GRELHA           180275 non-null object\n",
      "TEM_TAMPA            180275 non-null float64\n",
      "TIPO_PRODUTO         180275 non-null object\n",
      "TIPO_WOK             180275 non-null object\n",
      "ITEM_PRICE           167178 non-null float64\n",
      "INTERESTED           180275 non-null float64\n",
      "dtypes: float64(9), int64(1), object(10)\n",
      "memory usage: 28.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Float64Index: 27564 entries, 14.583125013599998 to 346.530445031\n",
      "Data columns (total 20 columns):\n",
      "ITEM_ID              27564 non-null int64\n",
      "ALTURA               27564 non-null float64\n",
      "CAPACIDADE_(L)       27564 non-null float64\n",
      "COMPOSICAO           27564 non-null object\n",
      "COR                  27564 non-null object\n",
      "FORMATO              27564 non-null object\n",
      "LARGURA              27564 non-null float64\n",
      "MARCA                27564 non-null object\n",
      "PARA_LAVA_LOUCAS     27564 non-null object\n",
      "PARA_MICRO_ONDAS     27564 non-null object\n",
      "PESO                 27564 non-null float64\n",
      "PROFUNDIDADE         27564 non-null float64\n",
      "TEMPO_GARANTIA       27564 non-null float64\n",
      "TEM_FERRO_FUNDIDO    27564 non-null object\n",
      "TEM_GRELHA           27564 non-null object\n",
      "TEM_TAMPA            27564 non-null float64\n",
      "TIPO_PRODUTO         27564 non-null object\n",
      "TIPO_WOK             27564 non-null object\n",
      "ITEM_PRICE           27564 non-null float64\n",
      "INTERESTED           27564 non-null float64\n",
      "dtypes: float64(9), int64(1), object(10)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "cleared_df = df.dropna()\n",
    "cleared_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27564\n"
     ]
    }
   ],
   "source": [
    "cleared_df.shape[0]/df.shape[0]*100\n",
    "print(cleared_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling the NANs\n",
    "If we simply cleared all the NaN we would lose 85% of the information so we need to see how to solve the NAN issue for each column. The information we have is the ITEM_ID, we will try to get the information for each type of item and fill it in the missing value. I'm assuming that the ITEM_ID is the product type so for the same product type the characteristics would be the same. This logic would not be used for price because this can change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "item_IDS =  df.ITEM_ID.unique()\n",
    "print(len(item_IDS))\n",
    "total_updated = 0\n",
    "for id in item_IDS:\n",
    "    filtered = df.loc[df['ITEM_ID'] == id].head().dropna()\n",
    "    if(filtered.size!=0):\n",
    "       total_updated +=1\n",
    "    \n",
    "    \n",
    "print(total_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the Data above we can see that only 31 products have all the information so we need to find another strategy to fill in the NaN. Also we can see if we would drop the NAN rows we would have information only about 31 products\n",
    "\n",
    "The First step will be for each column that has a NaN value we will create another categoric column, that will indicate for us if the product has or not the information related with the column. This will help the algoritm to understand if the information is relevant on the classification. After that the NAN will be replaced by zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALTURA', 'CAPACIDADE_(L)', 'COMPOSICAO', 'COR', 'FORMATO', 'LARGURA', 'MARCA', 'PARA_LAVA_LOUCAS', 'PARA_MICRO_ONDAS', 'PESO', 'PROFUNDIDADE', 'TEMPO_GARANTIA', 'ITEM_PRICE']\n",
      "['ITEM_ID', 'TEM_FERRO_FUNDIDO', 'TEM_GRELHA', 'TEM_TAMPA', 'TIPO_PRODUTO', 'TIPO_WOK', 'INTERESTED']\n"
     ]
    }
   ],
   "source": [
    "columns_with_missing_info = []\n",
    "columns_with_all_info = []\n",
    "for column in df.columns:\n",
    "    if(df[column].isna().sum()>0):\n",
    "        columns_with_missing_info.append(column)\n",
    "        new_column = column+\"_CONTAINS_INFO\"\n",
    "        df[new_column] = df[column].isna()\n",
    "    else:\n",
    "        if (column.find(\"_CONTAINS_INFO\")==-1):\n",
    "            columns_with_all_info.append(column)\n",
    "\n",
    "print(columns_with_missing_info)\n",
    "print(columns_with_all_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 6 numerical features missing information \n",
    "- ALTURA, CAPACIDADE_(L),LARGURA,PESO,PROFUNDIDADE,TEMPO_GARANTIA,ITEM_PRICE\n",
    "\n",
    "We have 5 categorical features missing information \n",
    "- COMPOSICAO, COR, FORMATO, MARCA, PARA_LAVA_LOUCAS, PARA_MICRO_ONDAS\n",
    "\n",
    "We are ignoring SESSION_ID for now because that information will be used later. For the missing information on the numerical columns we will use a LINEAR REGRESSION to infer de value for NAN. So in the following steps we will create the linear regression for each missing of those. The same process will be used on he Categorical Data. However to use the build in interpolation feature we have to put all the lines containing all the information at the begining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols= ['COMPOSICAO','COR','FORMATO','MARCA','PARA_LAVA_LOUCAS','PARA_MICRO_ONDAS','TEM_FERRO_FUNDIDO','TEM_GRELHA','TEM_TAMPA','TIPO_PRODUTO','TIPO_WOK']\n",
    "for col in categorical_cols: \n",
    "    df.sort_values(by=col,axis=0,ascending=False,inplace=True)\n",
    "    df[col] = df[col].astype('category')\n",
    "    df[col]= (df[col].cat.codes.replace(-1, np.nan).interpolate().astype(int).astype('category').cat.rename_categories(df[col].cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['ALTURA', 'CAPACIDADE_(L)','LARGURA','PESO','PROFUNDIDADE','TEMPO_GARANTIA','ITEM_PRICE']\n",
    "for col in numerical_cols: \n",
    "    df.sort_values(by=col,axis=0,ascending=False,inplace=True)\n",
    "    df[col]= df[col].interpolate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling the System\n",
    "Now that we have the dataset clean and without NaN we can start to try to use ML algorithms to predict the data. First we need to encode the categorical columns using one_hot encoding to avoid missing interpretation of the data based on the int values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_reg = pd.get_dummies(df, columns=categorical_cols).copy(deep=True)\n",
    "df_for_reg = df_for_reg.drop(columns=['ITEM_ID'])\n",
    "df_for_reg = df_for_reg.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split  \n",
    "regressors = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the data set in a simple MultiLayerPerceptron to see the acuracy without any fine tunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39437691\n",
      "Iteration 2, loss = 0.33919456\n",
      "Iteration 3, loss = 0.33255438\n",
      "Iteration 4, loss = 0.34675935\n",
      "Iteration 5, loss = 0.33608540\n",
      "Iteration 6, loss = 0.33665316\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "INTERESTED Accuracy: 0.9135547715724778\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_for_reg.sort_index()\n",
    "\n",
    "classifier = MLPClassifier(verbose=True)  \n",
    "y = df_for_reg['INTERESTED'].values\n",
    "x = df_for_reg.loc[:, df_for_reg.columns != 'INTERESTED'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y)  \n",
    "classifier.fit(X_train, y_train)  \n",
    "result =classifier.score(X_test,y_test)\n",
    "print(column+\" Accuracy: \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any fine tunning we have a accuracy of 91%. Let See the Parameters of the current MLP to see how it's set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will try to change de number of nodes in the hidden layer to see if there is any improvment. Based on the results we will check the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8928753688788302\n",
      "Iteration: 0 HiddenLayers: {67}\n",
      "Accuracy: 0.9147529343894917\n",
      "Iteration: 1 HiddenLayers: {97}\n",
      "Accuracy: 0.9157292152033548\n",
      "Iteration: 2 HiddenLayers: {36}\n"
     ]
    }
   ],
   "source": [
    "features= len(df_for_reg.columns)\n",
    "bestHidden = ()\n",
    "bestAc = 0;\n",
    "for i in range(1,3):\n",
    "    for j in range(5):        \n",
    "        hidden_layers = set(np.random.randint(features*0.20,high=features*0.80,size=(1,i))[0])       \n",
    "        classifier = MLPClassifier(hidden_layer_sizes=hidden_layers)\n",
    "        y = df_for_reg['INTERESTED'].values\n",
    "        x = df_for_reg.loc[:, df_for_reg.columns != 'INTERESTED'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y)  \n",
    "        classifier.fit(X_train, y_train)  \n",
    "        result =classifier.score(X_test,y_test)\n",
    "        if(bestAc<result):\n",
    "            bestAc = result\n",
    "            bestHidden = hidden_layers        \n",
    "            print(\"Accuracy: \" + str(result))\n",
    "            print(\"Iteration: \"+ str(j) +\" HiddenLayers: \" + str(hidden_layers))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no great improvment on the accuracy on this path so we will keep on 2 layers (features, features/2). Now we will try to improve the parameters of the MLP. We will keep the ReLu because itś the most modern activation function. But we will change the rest to achieve a better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39159638\n",
      "Iteration 2, loss = 0.30970653\n",
      "Iteration 3, loss = 0.29814838\n",
      "Iteration 4, loss = 0.29586424\n",
      "Iteration 5, loss = 0.29519062\n",
      "Iteration 6, loss = 0.29494334\n",
      "Iteration 7, loss = 0.29482278\n",
      "Iteration 8, loss = 0.29475927\n",
      "Iteration 9, loss = 0.29471616\n",
      "Iteration 10, loss = 0.29467021\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Accuracy: 0.9113359515409705\n"
     ]
    }
   ],
   "source": [
    "features= len(df_for_reg.columns)\n",
    "hidden_layers = (features, int(features*0.75))\n",
    "classifier_invscaling = MLPClassifier(verbose=True, hidden_layer_sizes=hidden_layers,learning_rate='invscaling',solver='sgd')\n",
    "y = df_for_reg['INTERESTED'].values\n",
    "x = df_for_reg.loc[:, df_for_reg.columns != 'INTERESTED'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y)  \n",
    "classifier_invscaling.fit(X_train, y_train)  \n",
    "result =classifier_invscaling.score(X_test,y_test)\n",
    "print(\"Accuracy: \" + str(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.35926356\n",
      "Iteration 2, loss = 0.30201850\n",
      "Iteration 3, loss = 0.30561294\n",
      "Iteration 4, loss = 0.29969778\n",
      "Iteration 5, loss = 0.29962431\n",
      "Iteration 6, loss = 0.29810105\n",
      "Iteration 7, loss = 0.29621052\n",
      "Iteration 8, loss = 0.29524628\n",
      "Iteration 9, loss = 0.29505816\n",
      "Iteration 10, loss = 0.29520973\n",
      "Iteration 11, loss = 0.29326430\n",
      "Iteration 12, loss = 0.29481903\n",
      "Iteration 13, loss = 0.29276287\n",
      "Iteration 14, loss = 0.29266833\n",
      "Iteration 15, loss = 0.29555317\n",
      "Iteration 16, loss = 0.29246846\n",
      "Iteration 17, loss = 0.29130952\n",
      "Iteration 18, loss = 0.29111518\n",
      "Iteration 19, loss = 0.29061637\n",
      "Iteration 20, loss = 0.29171496\n",
      "Iteration 21, loss = 0.29035917\n",
      "Iteration 22, loss = 0.29034295\n",
      "Iteration 23, loss = 0.29051576\n",
      "Iteration 24, loss = 0.29030343\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 25, loss = 0.28868568\n",
      "Iteration 26, loss = 0.28833234\n",
      "Iteration 27, loss = 0.28806411\n",
      "Iteration 28, loss = 0.28805743\n",
      "Iteration 29, loss = 0.28785812\n",
      "Iteration 30, loss = 0.28781936\n",
      "Iteration 31, loss = 0.28787617\n",
      "Iteration 32, loss = 0.28745071\n",
      "Iteration 33, loss = 0.28757288\n",
      "Iteration 34, loss = 0.28738765\n",
      "Iteration 35, loss = 0.28752307\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 36, loss = 0.28701009\n",
      "Iteration 37, loss = 0.28672482\n",
      "Iteration 38, loss = 0.28664654\n",
      "Iteration 39, loss = 0.28664002\n",
      "Iteration 40, loss = 0.28653743\n",
      "Iteration 41, loss = 0.28652169\n",
      "Iteration 42, loss = 0.28650773\n",
      "Iteration 43, loss = 0.28651386\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 44, loss = 0.28640882\n",
      "Iteration 45, loss = 0.28638329\n",
      "Iteration 46, loss = 0.28636891\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 47, loss = 0.28633888\n",
      "Iteration 48, loss = 0.28633675\n",
      "Iteration 49, loss = 0.28633434\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 50, loss = 0.28632863\n",
      "Iteration 51, loss = 0.28632751\n",
      "Iteration 52, loss = 0.28632667\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Accuracy: 0.9148638753910671\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = (features, int(features/2))\n",
    "classifier_adaptive = MLPClassifier(verbose=True, hidden_layer_sizes=hidden_layers,learning_rate='adaptive',solver='sgd')\n",
    "y = df_for_reg['INTERESTED'].values\n",
    "x = df_for_reg.loc[:, df_for_reg.columns != 'INTERESTED'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y)  \n",
    "classifier_adaptive.fit(X_train, y_train)  \n",
    "result =classifier_adaptive.score(X_test,y_test)\n",
    "print(\"Accuracy: \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accurary was never over 91% so we need to go bacck and try to engeneer the features to see if we can improve on this algorithm. If that doesn't change we will try other algorithms. The first thing is transform the numerical features in buckets for categorization and after that try to apply the algorithms again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALTURA</th>\n",
       "      <th>CAPACIDADE_(L)</th>\n",
       "      <th>LARGURA</th>\n",
       "      <th>PESO</th>\n",
       "      <th>PROFUNDIDADE</th>\n",
       "      <th>TEMPO_GARANTIA</th>\n",
       "      <th>ITEM_PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>180275.000000</td>\n",
       "      <td>180275.000000</td>\n",
       "      <td>180275.000000</td>\n",
       "      <td>180275.000000</td>\n",
       "      <td>180275.000000</td>\n",
       "      <td>180275.000000</td>\n",
       "      <td>180275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.106552</td>\n",
       "      <td>1.826867</td>\n",
       "      <td>26.680072</td>\n",
       "      <td>170.390221</td>\n",
       "      <td>33.136967</td>\n",
       "      <td>9.765170</td>\n",
       "      <td>118.584038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.785682</td>\n",
       "      <td>2.349141</td>\n",
       "      <td>11.451800</td>\n",
       "      <td>1286.478914</td>\n",
       "      <td>16.415044</td>\n",
       "      <td>36.510763</td>\n",
       "      <td>136.281879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>100.339412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.700000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>32.700000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>139.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>44400.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>2274.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ALTURA  CAPACIDADE_(L)        LARGURA           PESO  \\\n",
       "count  180275.000000   180275.000000  180275.000000  180275.000000   \n",
       "mean       29.106552        1.826867      26.680072     170.390221   \n",
       "std        15.785682        2.349141      11.451800    1286.478914   \n",
       "min         2.000000        0.045000       4.500000       0.619000   \n",
       "25%        16.000000        0.045000      18.500000       0.619000   \n",
       "50%        28.000000        0.045000      26.000000      60.000000   \n",
       "75%        42.700000        4.500000      32.700000     160.000000   \n",
       "max        84.000000       11.000000      91.000000   44400.000000   \n",
       "\n",
       "        PROFUNDIDADE  TEMPO_GARANTIA     ITEM_PRICE  \n",
       "count  180275.000000   180275.000000  180275.000000  \n",
       "mean       33.136967        9.765170     118.584038  \n",
       "std        16.415044       36.510763     136.281879  \n",
       "min         2.500000        1.000000       0.550000  \n",
       "25%        18.500000        1.000000      60.900000  \n",
       "50%        36.500000        3.000000     100.339412  \n",
       "75%        47.000000       12.000000     139.900000  \n",
       "max       148.000000      300.000000    2274.990000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ALTURA_BIN'] = pd.cut(df['ALTURA'], [0,2,16,28,43,84],labels=[1,2,3,4,5])\n",
    "df['CAPACIDADE_BIN'] = pd.cut(df['CAPACIDADE_(L)'], [0,0.045,4.5,11],labels=[1,2,3])\n",
    "df['LARGURA_BIN'] = pd.cut(df['LARGURA'], [0,4.5,18.5,26,33,91],labels=[1,2,3,4,5])\n",
    "df['PESO_BIN'] = pd.cut(df['PESO'], [0,0.62,60,160,44400],labels=[1,2,3,4])\n",
    "df['PROFUNDIDADE_BIN'] = pd.cut(df['PROFUNDIDADE'], [0,2.5,18.5,36.5,47,148],labels=[1,2,3,4,5])\n",
    "df['TEMPO_GARANTIA_BIN'] = pd.cut(df['TEMPO_GARANTIA'], [0,1,3,12,300],labels=[1,2,3,4])\n",
    "df['ITEM_PRICE_BIN'] = pd.cut(df['ITEM_PRICE'], [0,0.55,61,100,139,2275],labels=[1,2,3,4,5])\n",
    "df_for_reg2 = df.copy(deep=True)\n",
    "df_for_reg2 = df_for_reg2.drop(columns = numerical_cols)\n",
    "df_for_reg2 = pd.get_dummies(df_for_reg2, columns=categorical_cols)\n",
    "df_for_reg2 = pd.get_dummies(df_for_reg2, columns=['ALTURA_BIN','CAPACIDADE_BIN','PESO_BIN','PROFUNDIDADE_BIN','TEMPO_GARANTIA_BIN','ITEM_PRICE_BIN'])\n",
    "df_for_reg2 = df_for_reg2.drop(columns=['ITEM_ID'])\n",
    "df_for_reg2 = df_for_reg2.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 HiddenLayers: {105}\n",
      "Accuracy: 0.911180634138765\n",
      "Iteration: 1 HiddenLayers: {65}\n",
      "Accuracy: 0.915174510195478\n",
      "Iteration: 2 HiddenLayers: {104, 55}\n",
      "Accuracy: 0.9152854511970534\n"
     ]
    }
   ],
   "source": [
    "features = df_for_reg2.shape[1]\n",
    "bestHidden = ()\n",
    "bestAc = 0\n",
    "for i in range(1,3):\n",
    "    for j in range(5):  \n",
    "        hidden_layers = set(np.random.randint(features*0.20,high=features*0.80,size=(1,i))[0])\n",
    "        \n",
    "        classifier = MLPClassifier(hidden_layer_sizes=hidden_layers)\n",
    "        y = df_for_reg['INTERESTED'].values.astype(float)\n",
    "        x = df_for_reg.loc[:, df_for_reg.columns != 'INTERESTED'].values.astype(float)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y)  \n",
    "        classifier.fit(X_train, y_train)  \n",
    "        result =classifier.score(X_test,y_test)\n",
    "        if(bestAc<result):\n",
    "            bestAc = result\n",
    "            bestHidden = hidden_layers        \n",
    "            print(\"Iteration: \"+ str(j) +\" HiddenLayers: \" + str(hidden_layers))\n",
    "            print(\"Accuracy: \" + str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(141, 70), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_adaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.28709777\n",
      "Iteration 2, loss = 0.28096773\n",
      "Iteration 3, loss = 0.27971715\n",
      "Iteration 4, loss = 0.27890857\n",
      "Iteration 5, loss = 0.27859683\n",
      "Iteration 6, loss = 0.27814625\n",
      "Iteration 7, loss = 0.27769963\n",
      "Iteration 8, loss = 0.27737633\n",
      "Iteration 9, loss = 0.27714248\n",
      "Iteration 10, loss = 0.27714530\n",
      "Iteration 11, loss = 0.27694294\n",
      "Iteration 12, loss = 0.27678021\n",
      "Iteration 13, loss = 0.27673154\n",
      "Iteration 14, loss = 0.27664676\n",
      "Iteration 15, loss = 0.27641854\n",
      "Iteration 16, loss = 0.27655044\n",
      "Iteration 17, loss = 0.27639124\n",
      "Iteration 18, loss = 0.27620903\n",
      "Iteration 19, loss = 0.27619239\n",
      "Iteration 20, loss = 0.27626690\n",
      "Iteration 21, loss = 0.27611291\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 22, loss = 0.27510346\n",
      "Iteration 23, loss = 0.27499877\n",
      "Iteration 24, loss = 0.27488554\n",
      "Iteration 25, loss = 0.27489688\n",
      "Iteration 26, loss = 0.27486033\n",
      "Iteration 27, loss = 0.27488271\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 28, loss = 0.27482180\n",
      "Iteration 29, loss = 0.27461616\n",
      "Iteration 30, loss = 0.27457140\n",
      "Iteration 31, loss = 0.27453099\n",
      "Iteration 32, loss = 0.27450925\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 33, loss = 0.27442603\n",
      "Iteration 34, loss = 0.27442016\n",
      "Iteration 35, loss = 0.27441846\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 36, loss = 0.27439895\n",
      "Iteration 37, loss = 0.27439863\n",
      "Iteration 38, loss = 0.27439804\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 39, loss = 0.27439372\n",
      "Iteration 40, loss = 0.27439352\n",
      "Iteration 41, loss = 0.27439344\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 42, loss = 0.27439260\n",
      "Iteration 43, loss = 0.27439256\n",
      "Iteration 44, loss = 0.27439256\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 45, loss = 0.27439238\n",
      "Iteration 46, loss = 0.27439238\n",
      "Iteration 47, loss = 0.27439238\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 48, loss = 0.27439234\n",
      "Iteration 49, loss = 0.27439234\n",
      "Iteration 50, loss = 0.27439234\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Accuracy: 0.9165501786150125\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = set([features,int(features*0.5)])\n",
    "y = df_for_reg2['INTERESTED'].values.astype(float)\n",
    "x = df_for_reg2.loc[:, df_for_reg2.columns != 'INTERESTED'].values.astype(float)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y)  \n",
    "classifier_adaptive = MLPClassifier(verbose=True, hidden_layer_sizes=hidden_layers,learning_rate_init=0.1,learning_rate='adaptive',solver='sgd')\n",
    "classifier_adaptive.fit(X_train, y_train)  \n",
    "result =classifier_adaptive.score(X_test,y_test)\n",
    "print(\"Accuracy: \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still didn't have any significant improvment. Let's try another Machine Learning Algorithm. Let's try SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(verbose=True).fit(X_train, y_train)\n",
    "result =svm.score(X_test,y_test)\n",
    "print(\"Accuracy: \" + str(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no significant difference in accuracy. However we can also encode de output in two classes, that way each outuput neuron can especialize in detect if the product is or not interessting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col= ['INTERESTED']\n",
    "df_for_reg3 = pd.get_dummies(df_for_reg2, columns=categorical_col).copy(deep=True)\n",
    "df_for_reg3 = df_for_reg3.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.57284854\n",
      "Iteration 2, loss = 0.55891534\n",
      "Iteration 3, loss = 0.55645983\n",
      "Iteration 4, loss = 0.55428710\n",
      "Iteration 5, loss = 0.55336890\n",
      "Iteration 6, loss = 0.55453189\n",
      "Iteration 7, loss = 0.55209392\n",
      "Iteration 8, loss = 0.55165622\n",
      "Iteration 9, loss = 0.55128167\n",
      "Iteration 10, loss = 0.55100058\n",
      "Iteration 11, loss = 0.55043589\n",
      "Iteration 12, loss = 0.55010905\n",
      "Iteration 13, loss = 0.54985535\n",
      "Iteration 14, loss = 0.54956129\n",
      "Iteration 15, loss = 0.55005336\n",
      "Iteration 16, loss = 0.54929882\n",
      "Iteration 17, loss = 0.54947089\n",
      "Iteration 18, loss = 0.54931627\n",
      "Iteration 19, loss = 0.54879274\n",
      "Iteration 20, loss = 0.54873773\n",
      "Iteration 21, loss = 0.54873763\n",
      "Iteration 22, loss = 0.54837140\n",
      "Iteration 23, loss = 0.54853804\n",
      "Iteration 24, loss = 0.54809671\n",
      "Iteration 25, loss = 0.54827781\n",
      "Iteration 26, loss = 0.54810817\n",
      "Iteration 27, loss = 0.54804169\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 28, loss = 0.54686825\n",
      "Iteration 29, loss = 0.54576585\n",
      "Iteration 30, loss = 0.54566428\n",
      "Iteration 31, loss = 0.54559502\n",
      "Iteration 32, loss = 0.54553060\n",
      "Iteration 33, loss = 0.54552190\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 34, loss = 0.54503432\n",
      "Iteration 35, loss = 0.54491536\n",
      "Iteration 36, loss = 0.54484863\n",
      "Iteration 37, loss = 0.54484257\n",
      "Iteration 38, loss = 0.54481925\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 39, loss = 0.54465129\n",
      "Iteration 40, loss = 0.54463893\n",
      "Iteration 41, loss = 0.54463258\n",
      "Iteration 42, loss = 0.54462772\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 43, loss = 0.54459063\n",
      "Iteration 44, loss = 0.54458974\n",
      "Iteration 45, loss = 0.54458793\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 46, loss = 0.54458007\n",
      "Iteration 47, loss = 0.54457989\n"
     ]
    }
   ],
   "source": [
    "resultcols =[k for k in df_for_reg3.columns if k.find(\"INTERESTED\")!=-1]\n",
    "y = df_for_reg3[resultcols].values\n",
    "x = df_for_reg3[df_for_reg3.columns.difference(resultcols)].values\n",
    "classifier = MLPClassifier(verbose=True, hidden_layer_sizes=(x.shape[1],int(x.shape[1]/2),int(x.shape[1]/4),int(x.shape[1]/8)),\n",
    "                           learning_rate_init = 0.1,\n",
    "                           learning_rate='adaptive',solver='sgd')  \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y)  \n",
    "classifier.fit(X_train, y_train)  \n",
    "result =classifier.score(X_test,y_test)\n",
    "print(\"Accuracy: \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no improvement as well, so we will try to implementa more complex NN in tensor flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlnd-tf-lab]",
   "language": "python",
   "name": "conda-env-dlnd-tf-lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
